minikube --container-runtime=containerd --cpus=4 --memory=8g --driver=kvm2 --network=default --profile demo start --wait=all
* [demo] minikube v1.35.0 on Ubuntu 24.04
* Using the kvm2 driver based on user configuration
* Starting "demo" primary control-plane node in "demo" cluster
* Creating kvm2 VM (CPUs=4, Memory=8192MB, Disk=20000MB) ...
* Preparing Kubernetes v1.32.0 on containerd 1.7.23 ...
  - Generating certificates and keys ...
  - Booting up control plane ...
  - Configuring RBAC rules ...
* Configuring bridge CNI (Container Networking Interface) ...
* Verifying Kubernetes components...
  - Using image gcr.io/k8s-minikube/storage-provisioner:v5
* Enabled addons: storage-provisioner, default-storageclass
* Done! kubectl is now configured to use "demo" cluster and "default" namespace by default
kubectl get pod -A
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE
kube-system   coredns-668d6bf9bc-l4lst       0/1     Running   0          6s
kube-system   etcd-demo                      1/1     Running   0          12s
kube-system   kube-apiserver-demo            1/1     Running   0          12s
kube-system   kube-controller-manager-demo   1/1     Running   0          12s
kube-system   kube-proxy-wtcgn               1/1     Running   0          7s
kube-system   kube-scheduler-demo            1/1     Running   0          12s
kube-system   storage-provisioner            1/1     Running   0          7s
minikube --profile demo ssh "ps -axww"
    PID TTY      STAT   TIME COMMAND
      1 ?        Ss     0:03 /sbin/init noembed norestore
      2 ?        S      0:00 [kthreadd]
      3 ?        I<     0:00 [rcu_gp]
      4 ?        I<     0:00 [rcu_par_gp]
      5 ?        I      0:00 [kworker/0:0-rcu_gp]
      6 ?        I<     0:00 [kworker/0:0H-events_highpri]
      7 ?        I      0:00 [kworker/u8:0-flush-253:0]
      8 ?        I<     0:00 [mm_percpu_wq]
      9 ?        S      0:00 [rcu_tasks_rude_]
     10 ?        S      0:00 [rcu_tasks_trace]
     11 ?        S      0:00 [ksoftirqd/0]
     12 ?        I      0:00 [rcu_sched]
     13 ?        S      0:00 [migration/0]
     14 ?        S      0:00 [cpuhp/0]
     15 ?        S      0:00 [cpuhp/1]
     16 ?        S      0:00 [migration/1]
     17 ?        S      0:00 [ksoftirqd/1]
     18 ?        I      0:00 [kworker/1:0-events]
     19 ?        I<     0:00 [kworker/1:0H-events_highpri]
     20 ?        S      0:00 [cpuhp/2]
     21 ?        S      0:00 [migration/2]
     22 ?        S      0:00 [ksoftirqd/2]
     23 ?        I      0:00 [kworker/2:0-cgroup_destroy]
     24 ?        I<     0:00 [kworker/2:0H-events_highpri]
     25 ?        S      0:00 [cpuhp/3]
     26 ?        S      0:00 [migration/3]
     27 ?        S      0:00 [ksoftirqd/3]
     28 ?        I      0:00 [kworker/3:0-mm_percpu_wq]
     29 ?        I<     0:00 [kworker/3:0H-events_highpri]
     30 ?        S      0:00 [kdevtmpfs]
     31 ?        I<     0:00 [netns]
     32 ?        S      0:00 [kauditd]
     33 ?        I      0:00 [kworker/0:1-mm_percpu_wq]
     34 ?        S      0:00 [oom_reaper]
     35 ?        I<     0:00 [writeback]
     36 ?        S      0:00 [kcompactd0]
     37 ?        SN     0:00 [khugepaged]
     43 ?        I      0:00 [kworker/1:1-events]
     51 ?        I<     0:00 [cryptd]
     63 ?        I<     0:00 [kblockd]
     64 ?        I<     0:00 [blkcg_punt_bio]
     65 ?        I<     0:00 [ata_sff]
     66 ?        I<     0:00 [md]
     67 ?        I      0:00 [kworker/2:1-events]
     68 ?        I<     0:00 [kworker/0:1H-kblockd]
     69 ?        I<     0:00 [rpciod]
     70 ?        I<     0:00 [kworker/u9:0-xprtiod]
     71 ?        I<     0:00 [xprtiod]
     72 ?        I<     0:00 [cfg80211]
     74 ?        I      0:00 [kworker/3:1-events]
     75 ?        S      0:00 [kswapd0]
     76 ?        I<     0:00 [nfsiod]
     77 ?        I<     0:00 [cifsiod]
     78 ?        I<     0:00 [smb3decryptd]
     79 ?        I<     0:00 [cifsfileinfoput]
     80 ?        I<     0:00 [cifsoplockd]
     81 ?        I<     0:00 [xfsalloc]
     82 ?        I<     0:00 [xfs_mru_cache]
     84 ?        I<     0:00 [acpi_thermal_pm]
     85 ?        I      0:00 [kworker/u8:1-events_unbound]
     86 ?        S      0:00 [hwrng]
     87 ?        S      0:00 [scsi_eh_0]
     88 ?        I<     0:00 [scsi_tmf_0]
     89 ?        S      0:00 [scsi_eh_1]
     90 ?        I<     0:00 [scsi_tmf_1]
     91 ?        I      0:00 [kworker/u8:2-flush-253:0]
     92 ?        I      0:00 [kworker/3:2-events]
     93 ?        I<     0:00 [dm_bufio_cache]
     94 ?        I<     0:00 [kmpathd]
     95 ?        I<     0:00 [kmpath_handlerd]
     96 ?        I<     0:00 [kworker/1:1H-kblockd]
     97 ?        I<     0:00 [ipv6_addrconf]
     98 ?        I<     0:00 [ceph-msgr]
    112 ?        I      0:00 [kworker/u8:3]
    151 ?        Ss     0:00 /usr/sbin/rpcbind -w -f
    152 ?        Ss     0:00 /usr/lib/systemd/systemd-journald
    163 ?        I      0:00 [kworker/1:2-events]
    168 ?        I<     0:00 [kworker/2:1H-kblockd]
    171 ?        Ss     0:00 /usr/lib/systemd/systemd-udevd
    173 ?        I<     0:00 [kworker/3:1H-kblockd]
    189 ?        Ss     0:00 /usr/lib/systemd/systemd-networkd
    191 ?        Ss     0:00 /usr/lib/systemd/systemd-resolved
    192 ?        Ssl    0:00 /usr/lib/systemd/systemd-timesyncd
    202 ?        I      0:00 [kworker/2:2-events]
    210 ?        I      0:00 [kworker/0:2-events]
    222 ?        Ss     0:00 /usr/sbin/acpid --foreground --netlink
    224 ?        Ss     0:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation --syslog-only
    225 tty1     Ss+    0:00 /sbin/agetty -o -p -- \u --noclear - linux
    232 ttyS0    Ss+    0:00 /sbin/agetty -o -p -- \u --keep-baud 115200,57600,38400,9600 - vt220
    245 ?        Ss     0:00 /usr/lib/systemd/systemd-logind
    290 ?        S      0:00 [jbd2/vda1-8]
    291 ?        I<     0:00 [ext4-rsv-conver]
    324 ?        Ss     0:00 sshd: /usr/sbin/sshd -D -e [listener] 0 of 10-100 startups
    349 ?        I      0:00 [kworker/1:3-events]
    379 ?        Ss     0:00 /usr/sbin/rpc.statd
    380 ?        Ss     0:00 /usr/sbin/rpc.mountd
    384 ?        I<     0:00 [kworker/u9:1]
    385 ?        S      0:00 [lockd]
    422 ?        S      0:00 [nfsd]
    423 ?        S      0:00 [nfsd]
    424 ?        S      0:00 [nfsd]
    425 ?        S      0:00 [nfsd]
    426 ?        S      0:00 [nfsd]
    427 ?        S      0:00 [nfsd]
    428 ?        S      0:00 [nfsd]
    429 ?        S      0:00 [nfsd]
    695 ?        Ssl    0:00 /usr/bin/containerd --root /mnt/vda1/var/lib/containerd
    922 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id e9334c988818c33264443752e0bdad543de0975865a67f14b65e598bc312e280 -address /run/containerd/containerd.sock
    937 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id c414641f381bef008a2490bdaaf9ab98079216c68c8168ac6243e93e03b014d3 -address /run/containerd/containerd.sock
    943 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 83126982791451b13a991119aec687d27c5c10dfb25fb222b09920d466db1ca3 -address /run/containerd/containerd.sock
    944 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id d548b216cb6d7982e2afec79dbc115f05ef3a4b683f4bba0af265ecea9246cfb -address /run/containerd/containerd.sock
    997 ?        Ss     0:00 /pause
   1019 ?        Ss     0:00 /pause
   1027 ?        Ss     0:00 /pause
   1036 ?        Ss     0:00 /pause
   1124 ?        Ssl    0:00 etcd --advertise-client-urls=https://192.168.122.155:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://192.168.122.155:2380 --initial-cluster=demo=https://192.168.122.155:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.122.155:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.122.155:2380 --name=demo --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --proxy-refresh-interval=70000 --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
   1139 ?        Ssl    0:00 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=false
   1164 ?        Ssl    0:02 kube-apiserver --advertise-address=192.168.122.155 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key
   1173 ?        Ssl    0:00 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/var/lib/minikube/certs/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=mk --cluster-signing-cert-file=/var/lib/minikube/certs/ca.crt --cluster-signing-key-file=/var/lib/minikube/certs/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=false --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --root-ca-file=/var/lib/minikube/certs/ca.crt --service-account-private-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true
   1249 ?        Ssl    0:00 /var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=demo --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.122.155
   1413 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 196cce21b98ae92e196b5ee0fdedbe4e3c63db8d6c05b1d5e4189a19f45dcc7d -address /run/containerd/containerd.sock
   1433 ?        Ss     0:00 /pause
   1465 ?        Ssl    0:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=demo
   1596 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 03e7fa59b9b03bab3a2fd75ccda333c7c06dec98728c44d6861a1c75e9cad21e -address /run/containerd/containerd.sock
   1618 ?        Ss     0:00 /pause
   1650 ?        Ssl    0:00 /storage-provisioner
   1740 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id d25f2ace2f6de5d4e58b890b708e3764271ecd8047dd90205203f16bf1911ac5 -address /run/containerd/containerd.sock
   1761 ?        Ss     0:00 /pause
   1793 ?        Ssl    0:00 /coredns -conf /etc/coredns/Corefile
   1821 ?        I      0:00 [kworker/2:3-mm_percpu_wq]
   1828 ?        Ss     0:00 sshd: docker [priv]
   1830 ?        S      0:00 sshd: docker@pts/0
   1831 pts/0    Rs+    0:00 ps -axww
# The following line can be used to work around the issue with the kvm2 driver
# sleep 60
helm upgrade -i olm oci://ghcr.io/cloudtooling/helm-charts/olm --debug --version 0.30.0 || kubectl explain OLMConfig
history.go:56: 2025-03-09 19:42:47.58685656 +0100 CET m=+0.030837024 [debug] getting history for release olm
Release "olm" does not exist. Installing it now.
install.go:225: 2025-03-09 19:42:47.589510904 +0100 CET m=+0.033491361 [debug] Original chart version: "0.30.0"
time="2025-03-09T19:42:47+01:00" level=debug msg=resolving host=ghcr.io
time="2025-03-09T19:42:47+01:00" level=debug msg="do request" host=ghcr.io request.header.accept="application/vnd.docker.distribution.manifest.v2+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.oci.image.manifest.v1+json, application/vnd.oci.image.index.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=HEAD url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:42:47+01:00" level=debug msg="fetch response received" host=ghcr.io response.header.content-length=73 response.header.content-type=application/json response.header.date="Sun, 09 Mar 2025 18:43:02 GMT" response.header.www-authenticate="Bearer realm=\"https://ghcr.io/token\",service=\"ghcr.io\",scope=\"repository:cloudtooling/helm-charts/olm:pull\"" response.header.x-github-request-id="E723:3C7C29:44E6CD6:45C756C:67CDE136" response.status="401 Unauthorized" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:42:47+01:00" level=debug msg=Unauthorized header="Bearer realm=\"https://ghcr.io/token\",service=\"ghcr.io\",scope=\"repository:cloudtooling/helm-charts/olm:pull\"" host=ghcr.io
time="2025-03-09T19:42:47+01:00" level=debug msg="do request" host=ghcr.io request.header.accept="application/vnd.docker.distribution.manifest.v2+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.oci.image.manifest.v1+json, application/vnd.oci.image.index.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=HEAD url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:42:48+01:00" level=debug msg="fetch response received" host=ghcr.io response.header.content-length=758 response.header.content-type=application/vnd.oci.image.manifest.v1+json response.header.date="Sun, 09 Mar 2025 18:43:02 GMT" response.header.docker-content-digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" response.header.docker-distribution-api-version=registry/2.0 response.header.etag="\"sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d\"" response.header.x-github-request-id="E723:3C7C29:44E6D61:45C75F5:67CDE136" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:42:48+01:00" level=debug msg=resolved desc.digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" host=ghcr.io
time="2025-03-09T19:42:48+01:00" level=debug msg="do request" digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" request.header.accept="application/vnd.oci.image.manifest.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=GET url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d"
time="2025-03-09T19:42:48+01:00" level=debug msg="fetch response received" digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" response.header.content-length=758 response.header.content-type=application/vnd.oci.image.manifest.v1+json response.header.date="Sun, 09 Mar 2025 18:43:02 GMT" response.header.docker-content-digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" response.header.docker-distribution-api-version=registry/2.0 response.header.etag="\"sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d\"" response.header.x-github-request-id="E723:3C7C29:44E6DDC:45C7671:67CDE136" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d"
time="2025-03-09T19:42:48+01:00" level=debug msg="do request" digest="sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e" request.header.accept="application/vnd.cncf.helm.chart.content.v1.tar+gzip, */*" request.header.user-agent=Helm/3.17.1 request.method=GET url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e"
time="2025-03-09T19:42:48+01:00" level=debug msg="do request" digest="sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a" request.header.accept="application/vnd.cncf.helm.config.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=GET url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a"
time="2025-03-09T19:42:48+01:00" level=debug msg="fetch response received" digest="sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e" response.header.accept-ranges=bytes response.header.age=153 response.header.content-disposition= response.header.content-length=119252 response.header.content-type=application/octet-stream response.header.date="Sun, 09 Mar 2025 18:43:03 GMT" response.header.etag="\"0x8DD03050830065D\"" response.header.last-modified="Tue, 12 Nov 2024 10:30:30 GMT" response.header.server="Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0" response.header.strict-transport-security="max-age=31536000" response.header.via="1.1 varnish, 1.1 varnish" response.header.x-cache="MISS, HIT" response.header.x-cache-hits="0, 0" response.header.x-fastly-request-id=18e0245adb663f9f9dabf1360f91181675366680 response.header.x-ms-blob-type=BlockBlob response.header.x-ms-copy-completion-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-copy-id=7696dfc3-b3f2-4ecd-8e16-ca8235b14358 response.header.x-ms-copy-progress=119252/119252 response.header.x-ms-copy-status=success response.header.x-ms-creation-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-lease-state=available response.header.x-ms-lease-status=unlocked response.header.x-ms-request-id=51a151ce-801e-004a-0322-9116c0000000 response.header.x-ms-server-encrypted=true response.header.x-ms-version=2019-12-12 response.header.x-served-by="cache-iad-kcgs7200163-IAD, cache-fra-etou8220155-FRA" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e"
time="2025-03-09T19:42:48+01:00" level=debug msg="fetch response received" digest="sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a" response.header.accept-ranges=bytes response.header.age=153 response.header.content-disposition= response.header.content-length=288 response.header.content-type=application/octet-stream response.header.date="Sun, 09 Mar 2025 18:43:03 GMT" response.header.etag="\"0x8DD03050838C3DE\"" response.header.last-modified="Tue, 12 Nov 2024 10:30:30 GMT" response.header.server="Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0" response.header.strict-transport-security="max-age=31536000" response.header.via="1.1 varnish, 1.1 varnish" response.header.x-cache="MISS, HIT" response.header.x-cache-hits="0, 0" response.header.x-fastly-request-id=52c9f604188e44a80ba3f21435b7646b0b6bcd2b response.header.x-ms-blob-type=BlockBlob response.header.x-ms-copy-completion-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-copy-id=1a39b1c1-94fa-469f-80b7-e99e72dc3275 response.header.x-ms-copy-progress=288/288 response.header.x-ms-copy-status=success response.header.x-ms-creation-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-lease-state=available response.header.x-ms-lease-status=unlocked response.header.x-ms-request-id=1a21897f-b01e-003b-6b22-91fe69000000 response.header.x-ms-server-encrypted=true response.header.x-ms-version=2019-12-12 response.header.x-served-by="cache-iad-kiad7000094-IAD, cache-fra-etou8220155-FRA" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a"
time="2025-03-09T19:42:48+01:00" level=debug msg="encountered unknown type application/vnd.cncf.helm.config.v1+json; children may not be fetched"
time="2025-03-09T19:42:48+01:00" level=debug msg="encountered unknown type application/vnd.cncf.helm.chart.content.v1.tar+gzip; children may not be fetched"
Pulled: ghcr.io/cloudtooling/helm-charts/olm:0.30.0
Digest: sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d
install.go:242: 2025-03-09 19:42:48.769930255 +0100 CET m=+1.213910708 [debug] CHART PATH: /home/deas/.cache/helm/repository/olm-0.30.0.tgz

client.go:142: 2025-03-09 19:42:48.782579813 +0100 CET m=+1.226560269 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.827700563 +0100 CET m=+1.271681018 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.913552365 +0100 CET m=+1.357532827 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.935040105 +0100 CET m=+1.379020562 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.94654326 +0100 CET m=+1.390523722 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.958752784 +0100 CET m=+1.402733245 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.969540979 +0100 CET m=+1.413521437 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:42:48.986782121 +0100 CET m=+1.430762584 [debug] creating 1 resource(s)
wait.go:50: 2025-03-09 19:42:49.110217381 +0100 CET m=+1.554197838 [debug] beginning wait for 8 resources with timeout of 1m0s
install.go:212: 2025-03-09 19:42:51.176676612 +0100 CET m=+3.620657073 [debug] Clearing REST mapper cache
Error: unable to build kubernetes objects from release manifest: [resource mapping not found for name: "operatorhubio-catalog" namespace: "operator-lifecycle-manager" from "": no matches for kind "CatalogSource" in version "operators.coreos.com/v1alpha1"
ensure CRDs are installed first, resource mapping not found for name: "packageserver" namespace: "operator-lifecycle-manager" from "": no matches for kind "ClusterServiceVersion" in version "operators.coreos.com/v1alpha1"
ensure CRDs are installed first, resource mapping not found for name: "cluster" namespace: "" from "": no matches for kind "OLMConfig" in version "operators.coreos.com/v1"
ensure CRDs are installed first, resource mapping not found for name: "global-operators" namespace: "operators" from "": no matches for kind "OperatorGroup" in version "operators.coreos.com/v1"
ensure CRDs are installed first, resource mapping not found for name: "olm-operators" namespace: "operator-lifecycle-manager" from "": no matches for kind "OperatorGroup" in version "operators.coreos.com/v1"
ensure CRDs are installed first]
helm.go:86: 2025-03-09 19:42:51.378072404 +0100 CET m=+3.822052858 [debug] [resource mapping not found for name: "operatorhubio-catalog" namespace: "operator-lifecycle-manager" from "": no matches for kind "CatalogSource" in version "operators.coreos.com/v1alpha1"
ensure CRDs are installed first, resource mapping not found for name: "packageserver" namespace: "operator-lifecycle-manager" from "": no matches for kind "ClusterServiceVersion" in version "operators.coreos.com/v1alpha1"
ensure CRDs are installed first, resource mapping not found for name: "cluster" namespace: "" from "": no matches for kind "OLMConfig" in version "operators.coreos.com/v1"
ensure CRDs are installed first, resource mapping not found for name: "global-operators" namespace: "operators" from "": no matches for kind "OperatorGroup" in version "operators.coreos.com/v1"
ensure CRDs are installed first, resource mapping not found for name: "olm-operators" namespace: "operator-lifecycle-manager" from "": no matches for kind "OperatorGroup" in version "operators.coreos.com/v1"
ensure CRDs are installed first]
unable to build kubernetes objects from release manifest
helm.sh/helm/v3/pkg/action.(*Install).RunWithContext
	helm.sh/helm/v3/pkg/action/install.go:334
main.runInstall
	helm.sh/helm/v3/cmd/helm/install.go:317
main.newUpgradeCmd.func2
	helm.sh/helm/v3/cmd/helm/upgrade.go:160
github.com/spf13/cobra.(*Command).execute
	github.com/spf13/cobra@v1.8.1/command.go:985
github.com/spf13/cobra.(*Command).ExecuteC
	github.com/spf13/cobra@v1.8.1/command.go:1117
github.com/spf13/cobra.(*Command).Execute
	github.com/spf13/cobra@v1.8.1/command.go:1041
main.main
	helm.sh/helm/v3/cmd/helm/helm.go:85
runtime.main
	runtime/proc.go:272
runtime.goexit
	runtime/asm_amd64.s:1700
the server doesn't have a resource type "OLMConfig"
make: *** [Makefile:22: apply-demo] Error 1
minikube --profile demo delete
* Deleting "demo" in kvm2 ...
* Removed all traces of the "demo" cluster.
minikube --driver=docker --profile demo start --wait=all
* [demo] minikube v1.35.0 on Ubuntu 24.04
  - MINIKUBE_START_ARGS=--driver=docker
* Using the docker driver based on user configuration
* Using Docker driver with root privileges
* Starting "demo" primary control-plane node in "demo" cluster
* Pulling base image v0.0.46 ...
* Creating docker container (CPUs=2, Memory=7900MB) ...
* Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
  - Generating certificates and keys ...
  - Booting up control plane ...
  - Configuring RBAC rules ...
* Configuring bridge CNI (Container Networking Interface) ...
* Verifying Kubernetes components...
  - Using image gcr.io/k8s-minikube/storage-provisioner:v5
* Enabled addons: storage-provisioner, default-storageclass
* Done! kubectl is now configured to use "demo" cluster and "default" namespace by default
kubectl get pod -A
NAMESPACE     NAME                           READY   STATUS    RESTARTS      AGE
kube-system   coredns-668d6bf9bc-mqcz2       1/1     Running   0             11s
kube-system   etcd-demo                      1/1     Running   0             17s
kube-system   kube-apiserver-demo            1/1     Running   0             17s
kube-system   kube-controller-manager-demo   1/1     Running   0             17s
kube-system   kube-proxy-kjv6r               1/1     Running   0             11s
kube-system   kube-scheduler-demo            1/1     Running   0             17s
kube-system   storage-provisioner            1/1     Running   1 (10s ago)   11s
minikube --profile demo ssh "ps -axww"
    PID TTY      STAT   TIME COMMAND
      1 ?        Ss     0:00 /sbin/init
    104 ?        S<s    0:00 /lib/systemd/systemd-journald
    162 ?        Ss     0:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups
    967 ?        Ssl    0:00 /usr/bin/containerd
   1251 ?        Ssl    0:00 /usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12
   1526 ?        Ssl    0:00 /usr/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=registry.k8s.io/pause:3.10 --network-plugin=cni --hairpin-mode=hairpin-veth
   1966 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 82ab358175a0945141c4a71d12e480bec925fc5f45d3a97e530b3532badf39a5 -address /run/containerd/containerd.sock
   1967 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 5ed5fef69d0edd02f4ccc2a4665564ccb5d90afd6f1a5f8e8db7f0d41c232883 -address /run/containerd/containerd.sock
   2003 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 7406396c4a2c94b0ddaac0bbb9d1212bd7366a5e08f1e1278359c788e3231bf9 -address /run/containerd/containerd.sock
   2041 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id a9bd96dc1f08e87d72988eeaf079db77798adb8c6ffc61be413d0210bb4efc74 -address /run/containerd/containerd.sock
   2060 ?        Ss     0:00 /pause
   2063 ?        Ss     0:00 /pause
   2077 ?        Ss     0:00 /pause
   2085 ?        Ss     0:00 /pause
   2126 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 11d80b781b66399ee2790b5024929fe3591337a1aba7425e1176a77f531a7c70 -address /run/containerd/containerd.sock
   2157 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id a87c5b710a3e2df38b5d1f4d07d65cbf9547a14be9c12369cdad6e3a2a6805bf -address /run/containerd/containerd.sock
   2158 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id fcccc1dad6f283b5a6f3ef2ffc26a65b3753e74065c020154765c9bc51444a07 -address /run/containerd/containerd.sock
   2204 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 12970fe94b11dd912af02b96bc20e3ab4caee25eace583b9d7f14ca5ea230612 -address /run/containerd/containerd.sock
   2222 ?        Ssl    0:00 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/var/lib/minikube/certs/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=mk --cluster-signing-cert-file=/var/lib/minikube/certs/ca.crt --cluster-signing-key-file=/var/lib/minikube/certs/ca.key --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=false --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --root-ca-file=/var/lib/minikube/certs/ca.crt --service-account-private-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true
   2234 ?        Ssl    0:00 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=false
   2241 ?        Ssl    0:00 etcd --advertise-client-urls=https://192.168.49.2:2379 --cert-file=/var/lib/minikube/certs/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/minikube/etcd --experimental-initial-corrupt-check=true --experimental-watch-progress-notify-interval=5s --initial-advertise-peer-urls=https://192.168.49.2:2380 --initial-cluster=demo=https://192.168.49.2:2380 --key-file=/var/lib/minikube/certs/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.168.49.2:2380 --name=demo --peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/var/lib/minikube/certs/etcd/peer.key --peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt --proxy-refresh-interval=70000 --snapshot-count=10000 --trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt
   2248 ?        Ssl    0:02 kube-apiserver --advertise-address=192.168.49.2 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/var/lib/minikube/certs/ca.crt --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota --enable-bootstrap-token-auth=true --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=8443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/minikube/certs/sa.pub --service-account-signing-key-file=/var/lib/minikube/certs/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/var/lib/minikube/certs/apiserver.crt --tls-private-key-file=/var/lib/minikube/certs/apiserver.key
   2357 ?        Ssl    0:00 /var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=demo --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2
   2652 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 2e1fcc8ce2af8b1b07903189b00297f0e22c64b34b5753e5ed98c8e555c0b162 -address /run/containerd/containerd.sock
   2672 ?        Ss     0:00 /pause
   2694 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 686df582d29dc8543301e4f382f1b4a84255a83b726e45a995d3f07d5d83eb25 -address /run/containerd/containerd.sock
   2721 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id a5461a2dadc59eef6b709a08fe93c0077f792f6ffc6eed5ba38b6fa38826f6c5 -address /run/containerd/containerd.sock
   2737 ?        Ss     0:00 /pause
   2752 ?        Ss     0:00 /pause
   2764 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 3ea2274e0c6819cd8f07ad2630f2440d2faa08d714c27948e24a0fa8e9b1dc72 -address /run/containerd/containerd.sock
   2793 ?        Ssl    0:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=demo
   2982 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 6c58e2248c60403cb8bd836e438519c9ada035c241fb6984d628ee68a011b0d9 -address /run/containerd/containerd.sock
   3014 ?        Ssl    0:00 /coredns -conf /etc/coredns/Corefile
   3134 ?        Sl     0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id ccc4ca5893234de73430f5da8e1dd2a644802031c7b290f447558a56df66ea37 -address /run/containerd/containerd.sock
   3154 ?        Ssl    0:00 /storage-provisioner
   3217 ?        Ss     0:00 sshd: docker [priv]
   3219 ?        S      0:00 sshd: docker@pts/1
   3220 pts/1    Rs+    0:00 ps -axww
# The following line can be used to work around the issue with the kvm2 driver
# sleep 60
helm upgrade -i olm oci://ghcr.io/cloudtooling/helm-charts/olm --debug --version 0.30.0 || kubectl explain OLMConfig
history.go:56: 2025-03-09 19:43:22.040285478 +0100 CET m=+0.030163702 [debug] getting history for release olm
Release "olm" does not exist. Installing it now.
install.go:225: 2025-03-09 19:43:22.04245292 +0100 CET m=+0.032331141 [debug] Original chart version: "0.30.0"
time="2025-03-09T19:43:22+01:00" level=debug msg=resolving host=ghcr.io
time="2025-03-09T19:43:22+01:00" level=debug msg="do request" host=ghcr.io request.header.accept="application/vnd.docker.distribution.manifest.v2+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.oci.image.manifest.v1+json, application/vnd.oci.image.index.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=HEAD url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:43:22+01:00" level=debug msg="fetch response received" host=ghcr.io response.header.content-length=73 response.header.content-type=application/json response.header.date="Sun, 09 Mar 2025 18:43:36 GMT" response.header.www-authenticate="Bearer realm=\"https://ghcr.io/token\",service=\"ghcr.io\",scope=\"repository:cloudtooling/helm-charts/olm:pull\"" response.header.x-github-request-id="F360:35320B:22C05A6:236C1D4:67CDE158" response.status="401 Unauthorized" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:43:22+01:00" level=debug msg=Unauthorized header="Bearer realm=\"https://ghcr.io/token\",service=\"ghcr.io\",scope=\"repository:cloudtooling/helm-charts/olm:pull\"" host=ghcr.io
time="2025-03-09T19:43:22+01:00" level=debug msg="do request" host=ghcr.io request.header.accept="application/vnd.docker.distribution.manifest.v2+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.oci.image.manifest.v1+json, application/vnd.oci.image.index.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=HEAD url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:43:22+01:00" level=debug msg="fetch response received" host=ghcr.io response.header.content-length=758 response.header.content-type=application/vnd.oci.image.manifest.v1+json response.header.date="Sun, 09 Mar 2025 18:43:37 GMT" response.header.docker-content-digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" response.header.docker-distribution-api-version=registry/2.0 response.header.etag="\"sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d\"" response.header.x-github-request-id="F360:35320B:22C0600:236C232:67CDE158" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/0.30.0"
time="2025-03-09T19:43:22+01:00" level=debug msg=resolved desc.digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" host=ghcr.io
time="2025-03-09T19:43:22+01:00" level=debug msg="do request" digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" request.header.accept="application/vnd.oci.image.manifest.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=GET url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d"
time="2025-03-09T19:43:22+01:00" level=debug msg="fetch response received" digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" response.header.content-length=758 response.header.content-type=application/vnd.oci.image.manifest.v1+json response.header.date="Sun, 09 Mar 2025 18:43:37 GMT" response.header.docker-content-digest="sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d" response.header.docker-distribution-api-version=registry/2.0 response.header.etag="\"sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d\"" response.header.x-github-request-id="F360:35320B:22C063C:236C274:67CDE159" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/manifests/sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d"
time="2025-03-09T19:43:22+01:00" level=debug msg="do request" digest="sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e" request.header.accept="application/vnd.cncf.helm.chart.content.v1.tar+gzip, */*" request.header.user-agent=Helm/3.17.1 request.method=GET url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e"
time="2025-03-09T19:43:22+01:00" level=debug msg="do request" digest="sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a" request.header.accept="application/vnd.cncf.helm.config.v1+json, */*" request.header.user-agent=Helm/3.17.1 request.method=GET url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a"
time="2025-03-09T19:43:23+01:00" level=debug msg="fetch response received" digest="sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a" response.header.accept-ranges=bytes response.header.age=188 response.header.content-disposition= response.header.content-length=288 response.header.content-type=application/octet-stream response.header.date="Sun, 09 Mar 2025 18:43:37 GMT" response.header.etag="\"0x8DD03050838C3DE\"" response.header.last-modified="Tue, 12 Nov 2024 10:30:30 GMT" response.header.server="Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0" response.header.strict-transport-security="max-age=31536000" response.header.via="1.1 varnish, 1.1 varnish" response.header.x-cache="MISS, HIT" response.header.x-cache-hits="0, 1" response.header.x-fastly-request-id=eb680cee64e95769babb3f96a002d05bf41cb63f response.header.x-ms-blob-type=BlockBlob response.header.x-ms-copy-completion-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-copy-id=1a39b1c1-94fa-469f-80b7-e99e72dc3275 response.header.x-ms-copy-progress=288/288 response.header.x-ms-copy-status=success response.header.x-ms-creation-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-lease-state=available response.header.x-ms-lease-status=unlocked response.header.x-ms-request-id=1a21897f-b01e-003b-6b22-91fe69000000 response.header.x-ms-server-encrypted=true response.header.x-ms-version=2019-12-12 response.header.x-served-by="cache-iad-kiad7000094-IAD, cache-fra-etou8220073-FRA" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:bfbf7b9214a8d57271c49335af7e72034a466ed244312fc26fff013834daa42a"
time="2025-03-09T19:43:23+01:00" level=debug msg="encountered unknown type application/vnd.cncf.helm.config.v1+json; children may not be fetched"
time="2025-03-09T19:43:23+01:00" level=debug msg="fetch response received" digest="sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e" response.header.accept-ranges=bytes response.header.age=188 response.header.content-disposition= response.header.content-length=119252 response.header.content-type=application/octet-stream response.header.date="Sun, 09 Mar 2025 18:43:37 GMT" response.header.etag="\"0x8DD03050830065D\"" response.header.last-modified="Tue, 12 Nov 2024 10:30:30 GMT" response.header.server="Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0" response.header.strict-transport-security="max-age=31536000" response.header.via="1.1 varnish, 1.1 varnish" response.header.x-cache="MISS, HIT" response.header.x-cache-hits="0, 1" response.header.x-fastly-request-id=a067c60aba09655a2f7e2ce63b0ba0894df8f235 response.header.x-ms-blob-type=BlockBlob response.header.x-ms-copy-completion-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-copy-id=7696dfc3-b3f2-4ecd-8e16-ca8235b14358 response.header.x-ms-copy-progress=119252/119252 response.header.x-ms-copy-status=success response.header.x-ms-creation-time="Tue, 12 Nov 2024 10:30:30 GMT" response.header.x-ms-lease-state=available response.header.x-ms-lease-status=unlocked response.header.x-ms-request-id=51a151ce-801e-004a-0322-9116c0000000 response.header.x-ms-server-encrypted=true response.header.x-ms-version=2019-12-12 response.header.x-served-by="cache-iad-kcgs7200163-IAD, cache-fra-etou8220073-FRA" response.status="200 OK" url="https://ghcr.io/v2/cloudtooling/helm-charts/olm/blobs/sha256:fc4a636ecf00953778ce054e5d2efcbf38474085e038c2240d8ecd5bfe1dab4e"
time="2025-03-09T19:43:23+01:00" level=debug msg="encountered unknown type application/vnd.cncf.helm.chart.content.v1.tar+gzip; children may not be fetched"
Pulled: ghcr.io/cloudtooling/helm-charts/olm:0.30.0
Digest: sha256:9f075d8cd4417b02f34abb288bf3a4a25b8ac45314be8350dff6dc47fe61bf3d
install.go:242: 2025-03-09 19:43:23.128139251 +0100 CET m=+1.118017472 [debug] CHART PATH: /home/deas/.cache/helm/repository/olm-0.30.0.tgz

client.go:142: 2025-03-09 19:43:23.140128422 +0100 CET m=+1.130006648 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.185522989 +0100 CET m=+1.175401213 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.251579248 +0100 CET m=+1.241457475 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.25932995 +0100 CET m=+1.249208176 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.267233355 +0100 CET m=+1.257111578 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.276422095 +0100 CET m=+1.266300318 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.28446898 +0100 CET m=+1.274347206 [debug] creating 1 resource(s)
client.go:142: 2025-03-09 19:43:23.302742921 +0100 CET m=+1.292621153 [debug] creating 1 resource(s)
wait.go:50: 2025-03-09 19:43:23.326157535 +0100 CET m=+1.316035759 [debug] beginning wait for 8 resources with timeout of 1m0s
install.go:212: 2025-03-09 19:43:25.388554113 +0100 CET m=+3.378432336 [debug] Clearing REST mapper cache
client.go:142: 2025-03-09 19:43:25.715012975 +0100 CET m=+3.704891207 [debug] creating 14 resource(s)
NAME: olm
LAST DEPLOYED: Sun Mar  9 19:43:25 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
catalog:
  commandArgs: --configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
  image:
    pullPolicy: Always
    ref: quay.io/operator-framework/olm:v0.30.0
  nodeSelector:
    kubernetes.io/os: linux
  opmImageArgs: --opmImage=quay.io/operator-framework/opm:latest
  replicaCount: 1
  resources:
    requests:
      cpu: 10m
      memory: 80Mi
  service:
    externalPort: metrics
    internalPort: 8080
  setWorkloadUserID: true
catalog_namespace: operator-lifecycle-manager
debug: false
imagestream: false
installType: upstream
minKubeVersion: 1.11.0
monitoring:
  enabled: false
  namespace: monitoring
namespace: operator-lifecycle-manager
namespace_psa:
  auditLevel: restricted
  auditVersion: latest
  enforceLevel: baseline
  enforceVersion: latest
  warnLevel: restricted
  warnVersion: latest
olm:
  image:
    pullPolicy: Always
    ref: quay.io/operator-framework/olm:v0.30.0
  nodeSelector:
    kubernetes.io/os: linux
  replicaCount: 1
  resources:
    requests:
      cpu: 10m
      memory: 160Mi
  service:
    externalPort: metrics
    internalPort: 8080
operator_namespace: operators
operator_namespace_psa:
  enforceLevel: baseline
  enforceVersion: latest
package:
  image:
    pullPolicy: Always
    ref: quay.io/operator-framework/olm:v0.30.0
  maxSurge: 1
  maxUnavailable: 1
  nodeSelector:
    kubernetes.io/os: linux
  replicaCount: 2
  resources:
    requests:
      cpu: 10m
      memory: 50Mi
  service:
    internalPort: 5443
rbacApiVersion: rbac.authorization.k8s.io
writeStatusName: '""'

HOOKS:
MANIFEST:
---
# Source: olm/templates/0000_50_olm_00-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: operator-lifecycle-manager
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest
---
# Source: olm/templates/0000_50_olm_00-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: operators
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest
---
# Source: olm/templates/0000_50_olm_01-olm-operator.serviceaccount.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: olm-operator-serviceaccount
  namespace: operator-lifecycle-manager
---
# Source: olm/templates/0000_50_olm_01-olm-operator.serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:controller:operator-lifecycle-manager
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["watch", "list", "get", "create", "update", "patch", "delete", "deletecollection", "escalate", "bind"]
- nonResourceURLs: ["*"]
  verbs: ["*"]
---
# Source: olm/templates/0000_50_olm_09-aggregated.clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aggregate-olm-edit
  labels:
    # Add these permissions to the "admin" and "edit" default roles.
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
- apiGroups: ["operators.coreos.com"]
  resources: ["subscriptions"]
  verbs: ["create", "update", "patch", "delete"]
- apiGroups: ["operators.coreos.com"]
  resources: ["clusterserviceversions", "catalogsources", "installplans", "subscriptions"]
  verbs: ["delete"]
---
# Source: olm/templates/0000_50_olm_09-aggregated.clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aggregate-olm-view
  labels:
    # Add these permissions to the "admin", "edit" and "view" default roles
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"
rules:
- apiGroups: ["operators.coreos.com"]
  resources: ["clusterserviceversions", "catalogsources", "installplans", "subscriptions", "operatorgroups"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["packages.operators.coreos.com"]
  resources: ["packagemanifests", "packagemanifests/icon"]
  verbs: ["get", "list", "watch"]
---
# Source: olm/templates/0000_50_olm_01-olm-operator.serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: olm-operator-binding-operator-lifecycle-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:controller:operator-lifecycle-manager
subjects:
- kind: ServiceAccount
  name: olm-operator-serviceaccount
  namespace: operator-lifecycle-manager
---
# Source: olm/templates/0000_50_olm_07-olm-operator.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: olm-operator
  namespace: operator-lifecycle-manager
  labels:
    app: olm-operator
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app: olm-operator
  template:
    metadata:
      labels:
        app: olm-operator
    spec:
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: olm-operator-serviceaccount
      containers:
        - name: olm-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: [ "ALL" ]
          command:
          - /bin/olm
          args:
          - --namespace
          - $(OPERATOR_NAMESPACE)
          - --writeStatusName
          - ""
          image: quay.io/operator-framework/olm:v0.30.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
              name: metrics
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
          terminationMessagePolicy: FallbackToLogsOnError
          env:
          - name: OPERATOR_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: OPERATOR_NAME
            value: olm-operator
          resources:
            requests:
              cpu: 10m
              memory: 160Mi
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: olm/templates/0000_50_olm_08-catalog-operator.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-operator
  namespace: operator-lifecycle-manager
  labels:
    app: catalog-operator
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      app: catalog-operator
  template:
    metadata:
      labels:
        app: catalog-operator
    spec:
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: olm-operator-serviceaccount
      containers:
        - name: catalog-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: [ "ALL" ]
          command:
          - /bin/catalog
          args:
          - '--namespace'
          - operator-lifecycle-manager
          - --configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest
          - --opmImage=quay.io/operator-framework/opm:latest
          - --util-image
          -  quay.io/operator-framework/olm:v0.30.0
          - --set-workload-user-id=true
          image: quay.io/operator-framework/olm:v0.30.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
              name: metrics
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
          terminationMessagePolicy: FallbackToLogsOnError
          resources:
            requests:
              cpu: 10m
              memory: 80Mi
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: olm/templates/0000_50_olm_17-upstream-operators.catalogsource.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: operatorhubio-catalog
  namespace: operator-lifecycle-manager
spec:
  sourceType: grpc
  image: quay.io/operatorhubio/catalog:latest
  displayName: Community Operators
  publisher: OperatorHub.io
  grpcPodConfig:
    securityContextConfig: restricted
  updateStrategy:
    registryPoll:
      interval: 60m
---
# Source: olm/templates/0000_50_olm_15-packageserver.clusterserviceversion.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: ClusterServiceVersion
metadata:
  name: packageserver
  namespace: operator-lifecycle-manager
  labels:
    olm.version: 0.30.0
spec:
  displayName: Package Server
  description: Represents an Operator package that is available from a given CatalogSource which will resolve to a ClusterServiceVersion.
  minKubeVersion: 1.11.0
  keywords: ['packagemanifests', 'olm', 'packages']
  maintainers:
  - name: Red Hat
    email: openshift-operators@redhat.com
  provider:
    name: Red Hat
  links:
  - name: Package Server
    url: https://github.com/operator-framework/operator-lifecycle-manager/tree/master/pkg/package-server
  installModes:
  - type: OwnNamespace
    supported: true
  - type: SingleNamespace
    supported: true
  - type: MultiNamespace
    supported: true
  - type: AllNamespaces
    supported: true
  install:
    strategy: deployment
    spec:
      clusterPermissions:
      - serviceAccountName: olm-operator-serviceaccount
        rules:
        - apiGroups:
            - authorization.k8s.io
          resources:
            - subjectaccessreviews
          verbs:
            - create
            - get
        - apiGroups:
          - ""
          resources:
          - configmaps
          verbs:
          - get
          - list
          - watch
        - apiGroups:
          - "operators.coreos.com"
          resources:
          - catalogsources
          verbs:
          - get
          - list
          - watch
        - apiGroups:
          - "packages.operators.coreos.com"
          resources:
          - packagemanifests
          verbs:
          - get
          - list
      deployments:
      - name: packageserver
        spec:
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxUnavailable: 1
              maxSurge: 1
          replicas: 2
          selector:
            matchLabels:
              app: packageserver
          template:
            metadata:
              labels:
                app: packageserver
            spec:
              securityContext:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
              serviceAccountName: olm-operator-serviceaccount
              nodeSelector:
                kubernetes.io/os: linux
              containers:
              - name: packageserver
                securityContext:
                  allowPrivilegeEscalation: false
                  capabilities:
                    drop: [ "ALL" ]
                command:
                - /bin/package-server
                - -v=4
                - --secure-port
                - "5443"
                - --global-namespace
                - operator-lifecycle-manager
                image: quay.io/operator-framework/olm:v0.30.0
                imagePullPolicy: Always
                ports:
                - containerPort: 5443
                  protocol: TCP
                livenessProbe:
                  httpGet:
                    scheme: HTTPS
                    path: /healthz
                    port: 5443
                readinessProbe:
                  httpGet:
                    scheme: HTTPS
                    path: /healthz
                    port: 5443
                terminationMessagePolicy: FallbackToLogsOnError
                resources:
                  requests:
                    cpu: 10m
                    memory: 50Mi
                volumeMounts:
                - name: tmpfs
                  mountPath: /tmp
              volumes:
              - name: tmpfs
                emptyDir: {}
  maturity: alpha
  version: 0.30.0
  apiservicedefinitions:
    owned:
    - group: packages.operators.coreos.com
      version: v1
      kind: PackageManifest
      name: packagemanifests
      displayName: PackageManifest
      description: A PackageManifest is a resource generated from existing CatalogSources and their ConfigMaps
      deploymentName: packageserver
      containerPort: 5443
---
# Source: olm/templates/0000_50_olm_02-olmconfig.yaml
apiVersion: operators.coreos.com/v1
kind: OLMConfig
metadata:
  name: cluster
---
# Source: olm/templates/0000_50_olm_13-operatorgroup-default.yaml
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: global-operators
  namespace: operators
---
# Source: olm/templates/0000_50_olm_13-operatorgroup-default.yaml
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: olm-operators
  namespace: operator-lifecycle-manager
spec:
  targetNamespaces:
    - operator-lifecycle-manager

minikube --profile demo delete
* Deleting "demo" in docker ...
* Deleting container "demo" ...
* Removing /home/deas/.minikube/machines/demo ...
* Removed all traces of the "demo" cluster.
